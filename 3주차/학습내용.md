# autograd

	자동 미분을 사용하여 신경망에서 역전파 단계의 연산을 자동화 할 수 있다.
	autograd를 사용할 때, 신경망의 순전파 단계는 연산 그래프를 정의하게 된다.
	이 그래프의 노드는 Tensor, 엣지는 입력 Tensor로부터 출력 Tensor를 만들어내는 함수. 
	autograd를 사용하여 역전파 단계를 계산한다. 이는 requires_grad=True를 갖는 모든 Tensor에 대해 손실의 변화도를 계산한다. 
	이후 w1.grad와 w2.grad는 w1과 w2 각각에 대한 손실의 변화도를 갖는 Tensor가 된다.
![autograd_ex_1](https://user-images.githubusercontent.com/72618459/97675814-118ff200-1ad3-11eb-9791-89cbe9b155d5.PNG)


# 새 autograd 함수 정의하기

	forward 함수는 입력 Tensor로부터 출력 Tensor를 계산한다. backward 함수는 어떤 스칼라 값에 대한 출력 Tensor의 변화도를 
	전달받고, 동일한 스칼라 값에 대한 입력 Tensor의 변화도를 계산한다.
![autograd_ex_2](https://user-images.githubusercontent.com/72618459/97675821-12c11f00-1ad3-11eb-9f5a-81d1008680b1.PNG)


# nn 모듈

	nn 패키지는 신경망 계층들과 거의 동일한 Module의 집합을 정의한다.
	Module은 입력 Tensor를 받고 출력 Tensor를 계산하는 한편, 학습 가능한 매개변수를 갖는 Tensor 같은 내부 상태를 갖는다. 
	nn.Sequential은 다른 Module들을 포함하는 Module로, 그 Module들을 순차적으로 적용하여 출력을 생성한다. 
	각각의 Linear Module은 선형 함수를 사용하여 입력으로부터 출력을 계산하고, 내부 Tensor에 가중치와 편향을 저장한다. 
	또한 nn 패키지에는 널리 사용하는 손실 함수들에 대한 정의도 포함한다. 
	Module 객체는 __call__ 연산자를 덮어써 함수처럼 호출할 수 있게 한다. 이렇게 함으로써 
	입력 데이터의 Tensor를 Module에 전달하여 출력 데이터의 Tensor를 생성한다.
![nn_ex_1](https://user-images.githubusercontent.com/72618459/97675827-148ae280-1ad3-11eb-968d-05ebe7ab5670.PNG)

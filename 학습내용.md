인공지능 공부를 통해 전반적인 내용은 이곳에 정리

# ReLU 신경망

  	ReLU는 Rectified linear Unit 이라는 뜻으로 정류된 선형 함수이다.
  	이 함수는 h(x) = x(x>0), 0(x<=0) 값을 나타내는 함수이다.
  	
	이 신경망은 하나의 은닉층을 갖고 있으며, 신경망의 출력과 정답 사이의 유클리드 거리를 최소화하는 식으로 
	경사하강법을 사용하여 무작위의 데이터를 맞추도록 학습할 것이다.
	
# 신경망

	신경망은 생물학 모델을 바탕으로한 컴퓨팅의 한 형태로서, layer로 조직된 많은 수의 처리 요소로 구성된 수학 모델, 
	외부 입력에 반응하여 동적으로 정보를 처리하는 간단하지만 고도로 상호 연결된 처리요소로 구성된 컴퓨터 시스템 등으로 정의된다.
	신경망은 Input(입력층), Hidden(은닉층), Output(출력층) 으로 표현할 수 있다. 
	은닉층은 양쪽의 입력층과 출력층과는 달리 우리눈에 보이지 않기 때문에 은닉 이라고 한다.
	
# 순전파, 역전파

	순전파는 뉴럴 네트워크 모델의 입력층부터 출력층까지 순서대로 변수들을 계산하고 저장하는 것을 의미한다. 
	역전파는 뉴럴 네트워크의 파라미터들에 대한 그래디언트를 계산하는 방법을 의미한다. 
	일반적으로 역전파는 뉴럴 네트워크의 각 층과 관련된 목적 함수의 중간 변수들과 파라미터들의 그래디언트를 
	출력층에서 입력층 순으로 계산하고 저장한다.

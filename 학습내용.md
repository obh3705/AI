인공지능 공부를 통해 전반적인 내용은 이곳에 정리

# ReLU 신경망

  	ReLU는 Rectified linear Unit 이라는 뜻으로 정류된 선형 함수이다.
  	이 함수는 h(x) = x(x>0), 0(x<=0) 값을 나타내는 함수이다.
  	
	이 신경망은 하나의 은닉층을 갖고 있으며, 신경망의 출력과 정답 사이의 유클리드 거리를 최소화하는 식으로 
	경사하강법을 사용하여 무작위의 데이터를 맞추도록 학습할 것이다.
	
# 신경망

	신경망은 생물학 모델을 바탕으로한 컴퓨팅의 한 형태로서, layer로 조직된 많은 수의 처리 요소로 구성된 수학 모델, 
	외부 입력에 반응하여 동적으로 정보를 처리하는 간단하지만 고도로 상호 연결된 처리요소로 구성된 컴퓨터 시스템 등으로 정의된다.
	신경망은 Input(입력층), Hidden(은닉층), Output(출력층) 으로 표현할 수 있다. 
	은닉층은 양쪽의 입력층과 출력층과는 달리 우리눈에 보이지 않기 때문에 은닉 이라고 한다.
	

# optim

	optim 패키지는 최적화 알고리즘에 대한 아이디어를 추상화하고 일반적으로 사용하는 최적화 알고리즘의 구현체를 제공한다. 
	optim 패키지를 사용하여 모델의 가중치를 갱신할 Optimizer를 정의한다. 
	역전파 단계 전에 Optimizer 객체를 사용하여 갱신할 변수들에 대한 모든 변화도를 0으로 만든다. 
	기본적으로 .backward()를 호출할 때마다 변화도가 버퍼에 누적되기 때문이다. 
	Optimizer의 step 함수를 호출하면 매개변수가 갱신된다.
![optim_ex_1](https://user-images.githubusercontent.com/72618459/97676171-91b65780-1ad3-11eb-8f6e-ad5587961445.PNG)

# nn.Module

	기존 모듈의 구성보다 더 복잡한 모델을 구성해야 할 때가 있다. 이럴 때에 nn.Module의 서브클래스로 새 모듈을 정의하고, 
	입력 Tensor를 받아 다른 모듈 또는 Tensor의 autograd 연산을 사용하여 출력 Tensor를 만드는 forward 를 정의한다. 
	SGD 생성자에 model.parameters()를 호출하면 모델의 멤버인 2개의 nn.Linear 모듈의 학습 가능한 매개변수들이 포함된다.
![nn Module_ex_1](https://user-images.githubusercontent.com/72618459/97676166-8fec9400-1ad3-11eb-9621-5db173cdf875.PNG)


# Control Flow + Weight Sharing

	제어 흐름 + 가중치 공유
	일반적인 Python 제어 흐름을 사용하여 반복을 구현할 수 있으며, 순전파 단계를 정의할 때 단지 동일한 Module을 
	여러번 재사용함으로써 내부 계층들 간의 가중치 공유를 구현할 수 있다. 
	모델의 순전파 단계에서, 무작위로 0, 1, 2 또는 3 중에 하나를 선택하고 은닉층을 계산하기 위해 
	여러번 사용한 midde_linear Module을 재사용한다. 각 순전파 단계는 동적 연산 그래프를 구성하기 때문에 
	모델의 순전파 단계를 정의할 때 반복문이나 조건문과 같은 일반적인 Python 제어 흐름 연산자를 사용할 수 있다. 
	연산 그래프를 정의할 때 동일 Module을 여러번 재사용하는 것이 완벽히 안전하다.
![Control Flow + Weight Sharing_ex_1](https://user-images.githubusercontent.com/72618459/97690586-83bb0380-1ae0-11eb-9e42-53ff313c0260.PNG)

인공지능 공부를 통해 전반적인 내용은 이곳에 정리

# ReLU 신경망

  	ReLU는 Rectified linear Unit 이라는 뜻으로 정류된 선형 함수이다.
  	이 함수는 h(x) = x(x>0), 0(x<=0) 값을 나타내는 함수이다.
  	
	이 신경망은 하나의 은닉층을 갖고 있으며, 신경망의 출력과 정답 사이의 유클리드 거리를 최소화하는 식으로 
	경사하강법을 사용하여 무작위의 데이터를 맞추도록 학습할 것이다.
	
# 신경망

	신경망은 생물학 모델을 바탕으로한 컴퓨팅의 한 형태로서, layer로 조직된 많은 수의 처리 요소로 구성된 수학 모델, 
	외부 입력에 반응하여 동적으로 정보를 처리하는 간단하지만 고도로 상호 연결된 처리요소로 구성된 컴퓨터 시스템 등으로 정의된다.
	신경망은 Input(입력층), Hidden(은닉층), Output(출력층) 으로 표현할 수 있다. 
	은닉층은 양쪽의 입력층과 출력층과는 달리 우리눈에 보이지 않기 때문에 은닉 이라고 한다.
	
# 순전파, 역전파

	순전파는 뉴럴 네트워크 모델의 입력층부터 출력층까지 순서대로 변수들을 계산하고 저장하는 것을 의미한다. 
	입력층 -> 은닉층 -> 출력층 순으로 순서대로 다음 층으로 이동한다. 순전파는 순서대로 진행되며 
	마지막에 결과 값이 나오기 때문에 결과를 이용해 가중치 조절을 할 수 없다. 
	활성화 함수, 은닉층의 수 등 딥 러닝 모델을 설계하고 나면 입력값은 입력층, 은닉층을 지나면서 
	각 층에서의 가중치와 함께 연산되며 출력층으로 향한다. 그리고 출력층에서 모든 연산을 마친 예측값이 나오게 된다. 
	이와 같이 입력층에서 출력층 방향으로 예측값의 연산이 진행되는 과정을 순전파라고 한다.
	
	역전파는 뉴럴 네트워크의 파라미터들에 대한 그래디언트를 계산하는 방법을 의미한다. 
	일반적으로 역전파는 뉴럴 네트워크의 각 층과 관련된 목적 함수의 중간 변수들과 파라미터들의 그래디언트를 
	출력층에서 입력층 순으로 계산하고 저장한다.

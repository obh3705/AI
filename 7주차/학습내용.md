7주차 (2020.11.14 ~ 2020.11.20)

# 임베딩 층 사용하기

	nn.Embedding()을 사용할 경우
	전처리는 동일한 과정을 거친다.
	nn.Embedding()을 사용하여 학습가능한 임베딩 테이블을 만든다.
		num_embeddings: 단어 집합의 크기.
		embedding_dim: 임베딩 할 벡터의 차원, 사용자가 정해주는 하이퍼 파라미터.
		padding_idx: 선택적으로 사용, 패딩을 위한 토큰의 인덱스.

# 실습
	6. nn.Embedding()을 사용하여 3차원의 임베딩 벡터로 단어 집합의 크기인 7개의 행을 가지는 임베딩 테이블을 생성하였다.
![6C](https://user-images.githubusercontent.com/72618459/99876456-1c552700-2c3a-11eb-8265-9599fd110eaa.PNG)
![6S](https://user-images.githubusercontent.com/72618459/99876457-1cedbd80-2c3a-11eb-87d5-87f7a73f81ba.PNG)

# 임베딩 적용

	이번 실습한 임베딩을 저번에 했던 테스트 데이터를 이용해 실습해보려했다.
	vocab = {word: i+2 for i, word in enumerate(word_set)}
	단어 집합의 각 단어에 고유한 정수 맵핑을 하였다.
	TypeError: unhashable type: 'list'
	이러한 오류가 나왔고, 이 오류를 생각해보니 word_set에 문장 하나하나의 리스트로 저장된 것이 문제인 것 같았다.
![7S](https://user-images.githubusercontent.com/72618459/99876458-1cedbd80-2c3a-11eb-9323-6e90e182a271.PNG)

	앞의 문제를 해결하기 위해 for 문을 하나 더 이용해 문장 하나하나의 단어를 word_set에 저장할 수 있게 되었다.
	for V in list:
    for n in V.split():
   ![8C](https://user-images.githubusercontent.com/72618459/99876460-1e1eea80-2c3a-11eb-8cc7-7829181fd104.PNG)

	torchtext 예제와 비슷한 성능을 내기 위해 embedding_dim = 32 로 지정하였고 VOCAB_SIZE를 vocab의 길이로 지정해 주었다. 대분류의 감정을 분류해보는 것을 우선으로 해보기 위해 NUM_CLASS=6으로 해주었다.
![9C](https://user-images.githubusercontent.com/72618459/99876461-1e1eea80-2c3a-11eb-8a23-d235c41d6435.PNG)

	사용자 함수 generate_batch()를 이용하여 data batches와 offsets를 생성한다.
	이 함수는 torch.utils.data.DataLoader 안에 있는 collate_fn으로 전달된다.
	collate_fn의 인풋은 batch_size 만큼의 크기를 갖는 tensors로 이루어진 list이고,
	collate_fn 함수는 mini_batch로 나눈다. collate_fn은 최고 레벨의 함수로 선언된다.
	이는 이 함수가 각 worker에서 사용가능하게 한다.

	원본 data batch input의 텍스트는 list로 감싸져있고, 하나의 tensor로 concat되어 nn.EmbeddingBag의 input이 된다. Offset은 text tensor 내 개별 sequence의 시작점의 인덱스를 나타내는 텐서이다.

	torchtext.datasets.TextClassificationDataset 의 데이터는 label/tokens의 튜플로 이루어진 리스트로, tokens은 sring tokens를 numericalizing 한 것이고, label은 integer이다. [(label1, tokens1), (label2, tokens2), ...]

	dataset 형식을 맞추기 위해 json 파일의 원하는 부분을 
	{'emotion':emotion, 'content1':content1, 'content2':content2, 'content3':content3, 'content4':content4, 'content5':content5, 'content6':content6}
	으로 나누어 dataset을 만들었다.

	오류가 해결되지 않아 처음부터 더 자세한 코드 분석을 했다.
	_setup_datasets 함수에서 dataset을 tar 형식으로 다운로드 받고, 압축 해제한다.
	train.csv와 test.csv를 지정한다.
	tokenizer 후 train.csv를 열고 문장을 토큰화 하고 연결한다. 여기에서 data -> (cls, tokens) 로 되어있고, labels -> (cls) 로 되어있다.
	그래서 데이터셋의 형식을 맞추기 위해 예제에서 cls는 4가지 분류로 나타나 있었기 때문에 cls를 감정코드 의 숫자 부분만을 사용하기로 생각했고, token를 토큰화 리스트로 만들어보기로 했다.

	실행해 본 결과 문제가 많아 코드가 진행되는 과정을 일일이 print해보고 중간 값을 알 수 있었다.
	VOCAB_SIZE를 word_set의 길이로 지정했었는데 이 부분이 대화 하나하나마다의 VOCAB을 저장하고 초기화했기 때문에 값이 원하는 결과와 다르게 나왔다.
	이를 해결하기 위해 대화마다의 text인 content_text와 대화마다의 word_set인 content_word_set을 따로 만들어 주었다.
	리스트를 .copy로 사용하여 값을 복사해 주었는데 리스트 안의 리스트로 저장이 되었다. 그래서 .copy를 for문으로 바꾸어 값을 복사해 주었다.

![1C](https://user-images.githubusercontent.com/72618459/99876827-e36a8180-2c3c-11eb-8948-dcc57290a3b6.PNG)

	데이터셋의 형식이 단순히 {"감정":"감정코드 숫자", "대화":"대화1" ...} 이러한 방식으로 저장하면 저장은 할 수 있었지만 형식이 맞지 않았다.
	그래서 예제의 데이터셋 형태를 보고 vocab의 단어들의 정수 맵핑된 숫자를 token_ids로 저장하고 token을 tensor 형태로 저장해 주었다.
	data는 전에 했던 방식으로 (감정코드의 숫자, tensor 형태의 데이터) 순으로 대화 하나마다의 리스트로 저장하였다. labels는 감정코드의 숫자부분을 추출하여 저장하였다.
![3S](https://user-images.githubusercontent.com/72618459/99876956-de5a0200-2c3d-11eb-9d31-b6dcb022ba04.PNG)
	
	값들을 확인해 보기 위해 테스트 데이터를 20 가지의 대화를 넣었고, 중간 과정들을 출력해 보았다.
	
![2C](https://user-images.githubusercontent.com/72618459/99876954-dd28d500-2c3d-11eb-92a6-412d732eeb4b.PNG)
![2S](https://user-images.githubusercontent.com/72618459/99876955-de5a0200-2c3d-11eb-9842-d24f03e266ab.PNG)

	데이터가 원하는 모양으로 쌓여지는 것을 볼 수 있었다.
	그렇게 데이터를 쌓고 20개의 대화의 단어 수를 출력해 보았다.
![4S vocabsize](https://user-images.githubusercontent.com/72618459/99876994-32fd7d00-2c3e-11eb-8b9e-f4339e27352a.PNG)
	vocabsize 는 526으로 약 526개의 토큰으로 구성되었다.

predict 함수를 사용하기 위해 text의 형식을 토큰화된 단어들을 vocab[token] 의 텐서 형식으로 저장해 주었다.
![5C](https://user-images.githubusercontent.com/72618459/99877058-ccc52a00-2c3e-11eb-9399-2dc047587b5e.PNG)
![5S](https://user-images.githubusercontent.com/72618459/99877059-cd5dc080-2c3e-11eb-865e-a430375383a9.PNG)

이러한 데이터 셋의 형태를 맞추어보면서 구조적으로 이해할 수 있게 되었다. 하지만 해커톤의 기간이 부족해 결과를 제출하지 못 하였다.
오류가 나오는 부분은 데이터를 학습하는 부분이였는데 그 부분의 이론적인 부분 숙지가 부족해 해결하지 못 한 것 같다.
그래서 다음에는 모델을 직접 만들고 그 모델을 학습시키는 방법에 대해 더 알아볼 계획이다.
